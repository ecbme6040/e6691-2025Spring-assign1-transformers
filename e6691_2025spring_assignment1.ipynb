{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wVIqsmmioMU"
   },
   "source": [
    "# Assignment 1\n",
    "## EECS E6691 2025 Spring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAHR-MJzjhp-"
   },
   "source": [
    "This notebook implements a simplified ***Detection Transformer (DETR)*** model in PyTorch. The goal is to\n",
    "- Introduce the principles and implementations of **Transformers** (which will be discussed in greater details by upcoming lectures);\n",
    "- Introduce the training and evaluation of **object detection** models;\n",
    "- Familiarize the usage of **PyTorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key References:**\n",
    "- ***DETR Paper***: N. Carion, “End-to-End Object Detection with Transformers,” arXiv.org, May 26, 2020. https://arxiv.org/abs/2005.12872\n",
    "- ***DETR Variants***: T. Shehzadi, “Object Detection with Transformers: A Review,” arXiv.org, Jun. 07, 2023. https://arxiv.org/abs/2306.04670.\n",
    "- ***Official Code Base***: https://github.com/facebookresearch/detr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GetcTaaQjBvs"
   },
   "source": [
    "## 0 - Setup\n",
    "\n",
    "The following cells setup the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24478,
     "status": "ok",
     "timestamp": 1679935699795,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "h19L50SJj1af",
    "outputId": "042a11a6-39a0-48b9-eca6-26e3a24837dd"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# install necessary packets\n",
    "pip install opencv-python\n",
    "pip install numpy matplotlib\n",
    "pip install torchmetrics\n",
    "pip install transformers\n",
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V6HY-UJiv24"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# download the dataset\n",
    "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
    "unzip PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGF_hHuui2F4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "POWXsCOIED5w",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PENNFUDAN_LABEL_NAMES = [\n",
    "    '__background__', 'person'\n",
    "]\n",
    "COCO_LABEL_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0RZJLLTjJFe"
   },
   "source": [
    "## 1 - Dataset & Loader\n",
    "\n",
    "Below is a dataset for the Penn-Fudan Database for Pedestrian Detection and Segmentation.\n",
    "\n",
    "<font color=\"red\">***NOTE:***</font> Pay special attention to how the images are transformed and how the bounding boxes are obtained. Be very careful and make modifications (if necessary) when designing your own data augmentation pipelines.\n",
    "\n",
    "**Reference:** The code is modified from [Torchvision Object Detection Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRKPHFnTjIyu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PennFudanDataset(Dataset):\n",
    "    '''\n",
    "    Dataset\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    root -> str: root directory of the data folder\n",
    "    transforms -> callable: image transformation\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            # original annotation use the bottom-left corner as the origin\n",
    "            # here we reset it to start from top-left\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # normalize the boxes\n",
    "        width, height = img.size\n",
    "\n",
    "        # construct target (labels)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes / torch.tensor([width, height] * 2)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        # apply transformation\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        # return a tuple of (data, label)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5596,
     "status": "ok",
     "timestamp": 1679935715262,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "bZcAagkVi9N1",
    "outputId": "ae35f7da-1f3b-4a5f-a913-e54eba0d9434",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify transformations\n",
    "# here we only apply the basic conversions\n",
    "# other necessary transformations will be added later\n",
    "transform = transforms.Compose([\n",
    "    # convert to tensor\n",
    "    transforms.ToTensor(),\n",
    "    # convert to float\n",
    "    transforms.ConvertImageDtype(torch.float)\n",
    "])\n",
    "\n",
    "# construct dataset\n",
    "root = './PennFudanPed'\n",
    "dataset = PennFudanDataset(root, transform)\n",
    "\n",
    "# split the dataset to train and test sets\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_set = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "test_set = torch.utils.data.Subset(dataset, indices[-50:])\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's visualize some samples from the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNOY2bENjSz4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_boxes(img, bbs, conf=None, names=None, color=(1., 0, 0)):\n",
    "    '''\n",
    "    Draw bounding boxes given a pair of data points\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    image -> torch.Tensor: a 3D image tensor of shape (C, W, H)\n",
    "    bbs   -> torch.Tensor: a 2D tensor of shape (n_objs, 4), each one contains\n",
    "                           4 normalized box coordinates [tlx, tly, brx, bry]\n",
    "    conf  -> torch.Tensor: a 1D tensor of shape (n_objs,), the ith element  \n",
    "                           corresponds to the confidence score of the ith box\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    img -> numpy.array: the image with boxes overlaid\n",
    "    '''\n",
    "\n",
    "    # convert input to numpy arrays\n",
    "    # need a copy of the image to function correctly\n",
    "    if isinstance(img, torch.Tensor):\n",
    "        img = img.numpy().transpose(1, 2, 0).copy()\n",
    "    bbs = bbs.numpy()\n",
    "    width, height = img.shape[:2]\n",
    "    bbs = np.floor(bbs * np.array([height, width] * 2))\n",
    "\n",
    "    # draw boxes\n",
    "    for i, b in enumerate(bbs):\n",
    "        # retrive the top-left & bottom-right corner\n",
    "        tlx, tly, brx, bry = int(b[0]), int(b[1]), int(b[2]), int(b[3])\n",
    "        # cv2 draws the boxes inplace instead of returning a new image\n",
    "        cv2.rectangle(img, (tlx, tly), (brx, bry), color=color, thickness=2)\n",
    "        txt = ''\n",
    "        if names is not None:\n",
    "            txt += names[i]\n",
    "        if conf is not None:\n",
    "            txt += ' ({:.2f})'.format(conf[i])\n",
    "        cv2.putText(img, txt, (tlx, tly - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, thickness=1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 11251,
     "status": "ok",
     "timestamp": 1679935726510,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "pAdrr6zSjNrS",
    "outputId": "bb3f50dc-b8af-43c2-8bc7-9973220c56f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(range(len(train_set)), 6, replace=False)\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "\n",
    "for ii, i in enumerate(idx):\n",
    "    Xt, yt = train_set[i]\n",
    "    img = draw_boxes(Xt, yt['boxes'], color=(0, 1., 0), names=[PENNFUDAN_LABEL_NAMES[i] for i in yt['labels']])\n",
    "    ax[ii//3][ii%3].imshow(img)\n",
    "    ax[ii//3][ii%3].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TLEAy3ljhkZ"
   },
   "source": [
    "## 2 - Construct The Model\n",
    "\n",
    "**Reference:** DETRCarion, Nicolas, et al. “End-to-End Object Detection with Transformers.” ArXiv.org, 28 May 2020, https://arxiv.org/abs/2005.12872.\n",
    "\n",
    "[***Original Code Base***](https://github.com/facebookresearch/detr): This model was designed and implemented by FAIR with well-organized source codes. We will be implementing our own simpified version of that. Nevertheless, reading through parts of the code base will provide you with good understanding of what it looks like to be an industrial-level project.\n",
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-20_at_9.17.39_PM_ZHS2kmV.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peGXXffDrtmb"
   },
   "source": [
    "### 2.1 - Preparation\n",
    "\n",
    "When designing a complex model, it is usually a good practice to go bit-by-bit with every sub-module.\n",
    "\n",
    "Here, we collect a training sample to validate the functionality of our model during development. Pay special attention to the output shape of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1679935726510,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "7c0hL1ruPi2l",
    "outputId": "0882862d-9231-4241-802b-c634ef3ab7f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select an input image size of 380x380\n",
    "# you can change this later\n",
    "scale = (380, 380)\n",
    "\n",
    "# standard mean/std for resnet model, which we will use as feature extractors\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# resize & normalize the input images\n",
    "t = transforms.Compose([\n",
    "    transforms.Resize(scale, antialias=True),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# get a sample from our training set\n",
    "Xt, yt = next(iter(train_set))\n",
    "Xt = t(Xt)\n",
    "\n",
    "print('Input image shape:', Xt.shape)\n",
    "print('Bounding boxes:\\n', yt['boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqiHDrXdtC0y"
   },
   "source": [
    "### 2.2 - Backbone\n",
    "\n",
    "***Feature extraction*** is a common intermediate objective shared by many deep learning tasks, where we ask the model (or a sub-moodule) to learn a ***good*** representation of the input data for some specific objectives. Indeed, this is a crucial challenge in-and-of-itself. A well estabilished solution is to utilize other models pretrained on large, general purpose datasets (e.g. [ImageNet](https://www.image-net.org)) and fine-tune it for custom purposes.\n",
    "\n",
    "Now, let's load an ImageNet pretrained [**ResNet18**](https://arxiv.org/abs/1512.03385) and observe its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "87b61c5e134047e2856ba0d87aa3e5dd",
      "38136311e80d466fb3d8730e0937242e",
      "2dfdfeef01754f50be422d5fa0c6f334",
      "96644c80cb7f42a585b461d70556dfb5",
      "2accce33fa2c4a88964414e4053d751a",
      "8c80796a2ac746e2b34059eb3aae1b23",
      "f4318be27fa5479bbb561a7a81ad29fd",
      "577dd9a0aa1f4029a732f371f78c911b",
      "26e5b622bc424436a4f39678948a4a68",
      "345d31326fc74969b96d12de96c5b3ea",
      "a6f80041dd8c4af08b081b464106bf86"
     ]
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1679935726762,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "bhsxnp53SrS4",
    "outputId": "fa2fb183-d319-455e-92a9-1e04f427f246",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load a pretrained resnet18\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "backbone = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# list all the layers\n",
    "list(enumerate(backbone.children()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grW9qwuK3d0R"
   },
   "source": [
    "The ResNet18 we use here is consisted of ***10*** cascading sub-modules:\n",
    "- ***Layer 0-3:*** global feature extraction\n",
    "- ***Layer 4-7:*** different levels of local feature extraction\n",
    "- <font color=\"red\">***Layer 8:***</font> spatial pooling to ***1x1*** output\n",
    "- ***Layer 9:*** classification projection (ImageNet has 1000 different classes)\n",
    "\n",
    "***Note that***:\n",
    "- For classification, the whole image will reduced by ***layer 8*** to ***1x1*** spatial dimension because we are only interested in the class of the object instead of its location. \n",
    "- For object detection, however, it is crucial to maintain the **spatial information** of the extracted features, which indicates that we should consider the intermediate outputs <font color=\"red\">***BEFORE***</font> they are fed into layer 8 & 9.\n",
    "\n",
    "Therefore, we create a simple `Featurizer`, which acts as a wrapper that helps to skip the unwanted layers in the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1215,
     "status": "ok",
     "timestamp": 1679935727974,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "bOAU-9JwjZzj",
    "outputId": "9c9fa256-1755-4316-ab6e-4fb9fd3ea394",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Featurizer(nn.Module):\n",
    "    def __init__(self, model, weights, num_layers=6):\n",
    "        super().__init__()\n",
    "        model = model(weights=weights)\n",
    "\n",
    "        # to register a list of `nn.Module`s, wrap them up with `nn.ModuleList`:\n",
    "        # ref: https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\n",
    "        self.layers = nn.ModuleList(list(model.children()))\n",
    "        # compared to `nn.Sequential` that seals all modules as one, nn.ModuleList\n",
    "        # provides easier access to intermediate layers\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # feed forward through a specific number of layers\n",
    "        for layer in self.layers[:self.num_layers]:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# unit test\n",
    "featurizer = Featurizer(resnet18, ResNet18_Weights.DEFAULT, 7)\n",
    "feat = featurizer(Xt.unsqueeze(0))\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0S3NHsg-SSD"
   },
   "source": [
    "<font color=\"red\">***TODO:***</font> Observe the `Featurizer` output for all possible `num_layers = 1 ... 9` and choose a proper value for this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2273,
     "status": "ok",
     "timestamp": 1679935730244,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "e7RZbYmz3TfF",
    "outputId": "f61a2b76-0bb2-4bef-e302-7631761d5ba9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('The input shape is: {}\\n'.format(Xt.unsqueeze(0).shape))\n",
    "\n",
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "for i in range(9):\n",
    "    pass\n",
    "\n",
    "featurizer.num_layers = \n",
    "feat = featurizer(Xt.unsqueeze(0))\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################\n",
    "\n",
    "print('\\nThe chosen output shape is: {}, with {} layers of feature extraction'.format(feat.shape, featurizer.num_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jbn0OdFCFiae"
   },
   "source": [
    "### 2.3 - Multi-Head Attention\n",
    "\n",
    "The [original paper](https://arxiv.org/abs/2005.12872) (Section 2.1) provides a good motivation for using Transformers for object detection:\n",
    "\n",
    "> The first difficulty in these (*direct set prediction*) tasks is to avoid near-duplicates (*crowded bounding boxes*). Most current detectors use postprocessings such as non-maximal suppression to address this issue, but direct set prediction are postprocessing-free. They need global inference schemes that model interactions between all predicted elements to avoid redundancy. A general approach is to use auto-regressive sequence models such as recurrent neural networks. In contrast to most prior work however, we step away from autoregressive models and use transformers with parallel decoding.\n",
    "\n",
    "Consider some queries $Q \\in R^{M \\times d}$, keys $K \\in R^{N \\times d}$ and values $V \\in R^{N \\times D}$, the multi-head attention function with $h$ heads (see **Section 7.1-7.2**) is defined as\n",
    "\n",
    "$$\n",
    "\\text{MHAtt } (Q, K, V, h) = \\sigma (\\frac{Q \\otimes_h K^T}{\\sqrt{d/h}}) \\otimes_h V\n",
    "$$\n",
    "\n",
    "where $\\otimes$ denotes the (batched) matrix multiplication (this is the regular matrix multiplication on the ***last two*** dimensions, then propagated through other dimensions).\n",
    "\n",
    "In the above equation, $M, N$ are often refered to as the ***index dimensions*** (along which attentions are computed), and $d, D$ the ***feature dimensions*** (representation of the data sample at each indexed position). We particularly regard $M, N$ as the spatial locations on the feature map and $d, D$ the ResNet18-extracted feature dimensions corresponding to those locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZwzJkePxyDd"
   },
   "source": [
    "<font color=\"red\">***TODO:***</font> Complete the `MultiHeadAttention` module. Given `q`, `k` and `v` of shape `[batch_size, len, dim]`:\n",
    "- Reshape `q`, `k` and `v` into `[batch_size, len, num_heads, dim_per_head]`\n",
    "- Compute the attention scores `att` with `q`, `k`\n",
    "- Apply `dropout` to the attention scores\n",
    "- Attend `att` to `v` and generate the output `out`\n",
    "- Reshape `out` back to `[batch_size, len, dim]`\n",
    "\n",
    "For $\\otimes$ operation, you may use\n",
    "- [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html), or\n",
    "- [`torch.tensordot`](https://pytorch.org/docs/stable/generated/torch.tensordot.html), or\n",
    "- [`torch.matmul`](https://pytorch.org/docs/stable/generated/torch.matmul.html)\n",
    "\n",
    "***HINT:*** Following are some popular implementations that you may refer to\n",
    "- [`pytorch-transformer/model.py`](https://github.com/hkproj/pytorch-transformer/blob/main/model.py#L83)\n",
    "- [`deepmind-research/perceiver.py`](https://github.com/deepmind/deepmind-research/blob/master/perceiver/perceiver.py#L33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1679935730544,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "4tdl-f76FmOg",
    "outputId": "f88998ce-98f2-4270-d014-2921af1f1441",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Multi-Head Attention\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    dim: input/output dimension\n",
    "    n_heads: number of attention heads (must be a dividend of dim)\n",
    "    dropout: dropout rate\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        if dim % n_heads != 0:\n",
    "            raise ValueError('dim = {} must be a dividend of n_heads = {}'.format(dim, n_heads))\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.w_q = nn.Linear(dim, dim)\n",
    "        self.w_k = nn.Linear(dim, dim)\n",
    "        self.w_v = nn.Linear(dim, dim)\n",
    "        self.w_o = nn.Linear(dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "\n",
    "    def forward(self, inputs_q, inputs_k, inputs_v):\n",
    "        # input projection\n",
    "        q = self.w_q(inputs_q)\n",
    "        k = self.w_k(inputs_k)\n",
    "        v = self.w_v(inputs_v)\n",
    "\n",
    "        ############################################################################\n",
    "        #                         START OF YOUR CODE                               #\n",
    "        ############################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################################################################\n",
    "        #                          END OF YOUR CODE                                #\n",
    "        ############################################################################\n",
    "\n",
    "        # output projection\n",
    "        out = self.w_o(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# construct kv by flattening H & W\n",
    "kv = feat.flatten(2, 3).permute(0, 2, 1)\n",
    "dim = kv.shape[-1]\n",
    "# construct q containing 4 random queries\n",
    "q = torch.randn((1, 4, dim))\n",
    "\n",
    "# unit test\n",
    "attention = MultiHeadAttention(dim, 8, 0.5)\n",
    "att = attention(q, kv, kv)\n",
    "print(q.shape, kv.shape, att.shape)\n",
    "assert att.shape == q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxFJJU-8CMAS"
   },
   "source": [
    "### 2.4 - Positional Encoding\n",
    "\n",
    "Attention mechnism bears two important traits:\n",
    "- ***Global Inference:*** attention operation equally attend to all indexed positions, while autoregressive models like `LSTM` easily struggle with long term memory\n",
    "- ***Permutational Invariance:*** The order of the entries in $Q$ and $K, V$ does not have an effect on the output (see **Section 7.3**)\n",
    "\n",
    "Intuitively for our cases, different positions along the index dimension should correspond to different spatial locations. This particular positional information will be ignored by the attention mechnism, which makes it necessary to explicitly encode this information into the input features.\n",
    "\n",
    "We do this by introducing prositional encodings. It is a function that assigns a distinct value for every location along the index dimension(s).\n",
    "\n",
    "**Reference:** Code modified from [DETR/models/position_encoding.py](https://github.com/facebookresearch/detr/blob/main/models/position_encoding.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1679935730544,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "bzI-F75APogT",
    "outputId": "e5d3e7ce-960a-4e87-fbcb-93cc8bfd2d0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionEmbeddingSine(nn.Module):\n",
    "    '''\n",
    "    This is a more standard version of the position embedding, very similar to the one\n",
    "    used by the Attention is all you need paper, generalized to work on images.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_pos_feats, temperature=10000, normalize=False, scale=None):\n",
    "        super().__init__()\n",
    "        self.num_pos_feats = num_pos_feats\n",
    "        self.temperature = temperature\n",
    "        self.normalize = normalize\n",
    "        if scale is not None and normalize is False:\n",
    "            raise ValueError(\"normalize should be True if scale is passed\")\n",
    "        if scale is None:\n",
    "            import math\n",
    "            scale = 2 * math.pi\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # build encoding for both x and y positions\n",
    "        y_embed = torch.arange(h, device=x.device)\n",
    "        x_embed = torch.arange(w, device=x.device)\n",
    "        if self.normalize:\n",
    "            eps = 1e-6\n",
    "            y_embed = y_embed / (h + eps) * self.scale\n",
    "            x_embed = x_embed / (w + eps) * self.scale\n",
    "\n",
    "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
    "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
    "\n",
    "        pos_x = x_embed.view(1, 1, -1, 1).repeat(b, h, 1, 1) / dim_t\n",
    "        pos_y = y_embed.view(1, -1, 1, 1).repeat(b, 1, w, 1) / dim_t\n",
    "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
    "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
    "        return pos\n",
    "\n",
    "\n",
    "# unit test\n",
    "pos_enc = PositionEmbeddingSine(128)\n",
    "pos = pos_enc(feat)\n",
    "pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xklv6z8B6_k"
   },
   "source": [
    "### 2.5 - Transformer\n",
    "\n",
    "Next we construct the `Transformer`.\n",
    "\n",
    "The architecture basically follows from the original Transformer.\n",
    "\n",
    "![](https://www.researchgate.net/publication/344197785/figure/fig2/AS:934416989843456@1599793779015/Transformer-model-architecture-described-in-Attention-Is-All-You-Need-6_Q640.jpg)\n",
    "\n",
    "**Reference:** Vaswani, Ashish, et al. “Attention Is All You Need.” ArXiv.org, 6 Dec. 2017, https://arxiv.org/abs/1706.03762."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHMjGwNOHZyE"
   },
   "source": [
    "#### Encoder\n",
    "\n",
    "First for the `Encoder`. Pay attention to the building blocks and their implementations.\n",
    "\n",
    "<font color=\"red\">***NOTE:***</font> When needed, write `a = a + b` instead of `a += b` to avoid an error. This is because `a += b` will modify tensor `a` ***in-place***, which is prohibited during Pytorch auto-differentiation mechnism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1498,
     "status": "ok",
     "timestamp": 1679935732040,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "Tbzqcc5lYrdq",
    "outputId": "0c0b2b2b-063e-4e49-a965-da6641203aff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    '''\n",
    "    Encoder Layer\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    dim: input/output dimension\n",
    "    n_heads: number of attention heads (must be a dividend of dim)\n",
    "    dropout: dropout rate\n",
    "    expand: feed forward expansion factor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim, n_heads, dropout, expand):\n",
    "        super().__init__()\n",
    "        # encoder self attention\n",
    "        self.attend = MultiHeadAttention(dim, n_heads, dropout)\n",
    "        # attention norm\n",
    "        self.norm_att = nn.LayerNorm(dim)\n",
    "        # output norm\n",
    "        self.norm_out = nn.LayerNorm(dim)\n",
    "        # attention dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # feed forward mlp\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * expand),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * expand, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    # since this is a self-attention, there's no need to\n",
    "    # differentiate between q, k and v inputs\n",
    "    def forward(self, qkv, pos):\n",
    "        # compute the self attention\n",
    "        # note that posisional encoding is only applied to q, k\n",
    "        att = self.attend(qkv + pos, qkv + pos, qkv)\n",
    "\n",
    "        # add & norm\n",
    "        x = qkv + self.dropout(att)\n",
    "        x = self.norm_att(x)\n",
    "\n",
    "        # feed forward, add & norm\n",
    "        x = x + self.mlp(x)\n",
    "        x = self.norm_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' Encoder '''\n",
    "\n",
    "    def __init__(self, dim, n_layers, n_heads, dropout, expand):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(dim, n_heads, dropout, expand)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        # flatten the feature map and its positional encoding\n",
    "        x = x.flatten(2, 3).permute(0, 2, 1)\n",
    "        pos = pos.flatten(2, 3).permute(0, 2, 1)\n",
    "\n",
    "        # feed forward each layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, pos)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# unit test\n",
    "encoder = Encoder(256, 2, 8, 0.5, 2)\n",
    "z = encoder(feat, pos)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MRAdqEbI3ue"
   },
   "source": [
    "#### Decoder\n",
    "\n",
    "Next for the `Decoder`. The architecture and implementation is *almost* the same with `Encoder` with more variations. Specifically:\n",
    "- There are **TWO** attention blocks, one for **query self-attention**, the other for **encoder-decoder cross-attention**\n",
    "- Attention masking (for time sequence prediction) is not considered for this model\n",
    "- Queries of `Decoder` (not `DecoderLayer`) are simply the positional encodings (please be aware that this feature is a design of DETR, and you will see from later lectures that other models handles decoder queries in various different ways)\n",
    "\n",
    "As is provided in the code below, the third bulletin is implemented by setting the input decoder query `x` to zero, then add the passed-in argument `q` as query positional encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_TC7sY7NCl1"
   },
   "source": [
    "<font color=\"red\">***TODO:***</font> In the fashion of the `EcoderLayer`, implement the `DecoderLayer`.\n",
    "\n",
    "Feel free to reference the original code base\n",
    "- [`detr/transformer.py`](https://github.com/facebookresearch/detr/blob/main/models/transformer.py#L187)\n",
    "\n",
    "or other popular implementations like\n",
    "- [`pytorch-transformer/model.py`](https://github.com/hkproj/pytorch-transformer/blob/main/model.py#L161)\n",
    "- [`deepmind-research/perceiver.py`](https://github.com/deepmind/deepmind-research/blob/master/perceiver/perceiver.py#L521) (might not be straight forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1679935732040,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "DELSUo_1Chrg",
    "outputId": "9ea0c6aa-671c-47f4-83f8-af228b1fc812",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    '''\n",
    "    Decoder Layer\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    dim: input/output dimension\n",
    "    n_heads: number of attention heads (must be a dividend of dim)\n",
    "    dropout: dropout rate\n",
    "    expand: feed forward expansion factor\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim, n_heads, dropout, expand):\n",
    "        ############################################################################\n",
    "        #                         START OF YOUR CODE                               #\n",
    "        ############################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################################################################\n",
    "        #                          END OF YOUR CODE                                #\n",
    "        ############################################################################\n",
    "\n",
    "    # for cross-attention, q must be separated, while there's still \n",
    "    # no need to differentiate between k and v\n",
    "    def forward(self, q, kv, q_pos, kv_pos):\n",
    "        ############################################################################\n",
    "        #                         START OF YOUR CODE                               #\n",
    "        ############################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        ############################################################################\n",
    "        #                          END OF YOUR CODE                                #\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ''' Decoder '''\n",
    "\n",
    "    def __init__(self, dim, n_layers, n_heads, dropout, expand):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(dim, n_heads, dropout, expand) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, q, kv, pos):\n",
    "        pos = pos.flatten(2, 3).permute(0, 2, 1)\n",
    "        x = torch.zeros_like(q, device=q.device)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, kv, q, pos)\n",
    "        return x\n",
    "\n",
    "\n",
    "# unit test\n",
    "decoder = Decoder(256, 2, 8, 0.5, 2)\n",
    "res = decoder(q, z, pos)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9JVn7k-Mhox"
   },
   "source": [
    "#### Assemble\n",
    "\n",
    "Then we put them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">***TODO:***</font> There is an error in the following section that will fail the unit test. Correct the error and explain the reasoning behind this design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">***Answer:***</font> [Answer Here]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1679935732567,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "eX-nuCdYIZtL",
    "outputId": "6bcc3817-656e-4b6c-fe5b-13ca47e6a936",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    Transformer\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    dim: input/output dimension\n",
    "    n_heads: number of attention heads (must be a dividend of dim)\n",
    "    dropout: dropout rate\n",
    "    expand: feed forward expansion factor\n",
    "\n",
    "    pos_temp: positional encoding temperature\n",
    "    pos_norm: positional encoding normalization\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim, n_layers, n_heads, dropout, expand, pos_temp=10000, pos_norm=False):\n",
    "        super().__init__()\n",
    "        self.pos_enc = PositionEmbeddingSine(dim, pos_temp, pos_norm)\n",
    "        self.encoder = Encoder(dim, n_layers, n_heads, dropout, expand)\n",
    "        self.decoder = Decoder(dim, n_layers, n_heads, dropout, expand)\n",
    "\n",
    "    def forward(self, q, x):\n",
    "        pos = self.pos_enc(x)\n",
    "        z = self.encoder(x, pos)\n",
    "        out = self.decoder(q, z, pos)\n",
    "        return out\n",
    "\n",
    "\n",
    "# unit test\n",
    "transformer = Transformer(256, 2, 8, 0.5, 2)\n",
    "transformer(q, feat).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVl1FjG-RNbp"
   },
   "source": [
    "### 2.6 - DETR\n",
    "\n",
    "Finally we can build our version of the `DETR`. For each input image, predictions are generated by:\n",
    "- ***Feature Extraction***: featurize input data\n",
    "- ***Attention Operation***: transformer generate queried outputs\n",
    "- ***Projection***: for each queried output, project it individually to\n",
    "    - Class Probabilities (using a linear layer), and\n",
    "    - A Bounding Box (using a shallow `MLP`)\n",
    "\n",
    "Obviously, the quality of the queried outputs depends heavily on the selection of appropriate queries. In order to make life easier, we use a trainable positional encoding instead of the fixed one we used in encoder/decoder.\n",
    "\n",
    "While a naive [`torch.Parameter`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html) will suffice in constructing a trainable model parameter, [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) appears as a more decent choice here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">***TODO:***</font> Implement the `classifier` and the `regressor`. Following the proposed architecture in the paper,\n",
    "- Use a **Linear** layer for object classification\n",
    "- Use a shallow **MLP** bounding box regression\n",
    "\n",
    "Please reference the original code base for more details. Pay attention to the input/output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1679935732760,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "k8qxrbfl70bt",
    "outputId": "1d6ed2e2-577a-46c8-a005-6157888c2e99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DETR(nn.Module):\n",
    "    '''\n",
    "    DETR\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    dim: transformer dimension\n",
    "    n_classes: number of output classes\n",
    "    n_queries: number of queries\n",
    "    n_layers: number of encoder/decoder layers\n",
    "\n",
    "    featurizer_model: backbone model\n",
    "    featurizer_weights: backbone model weights\n",
    "    featurizer_layers: number of layers to keep in the backbone\n",
    "\n",
    "    n_heads: number of attention heads (must be a dividend of dim)\n",
    "    dropout: dropout rate\n",
    "    expand: feed forward expansion factor\n",
    "\n",
    "    pos_temp: positional encoding temperature\n",
    "    pos_norm: positional encoding normalization\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, dim, n_classes, n_queries, n_layers, featurizer_model, featurizer_weights,\n",
    "        featurizer_layers, n_heads=8, dropout=0.5, expand=2, pos_temp=10000, pos_norm=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.featurizer = Featurizer(featurizer_model, featurizer_weights, featurizer_layers)\n",
    "        self.transformer = Transformer(dim, n_layers, n_heads, dropout, expand, pos_temp, pos_norm)\n",
    "        # use learnable queries\n",
    "        self.query = nn.Embedding(n_queries, dim)\n",
    "        ############################################################################\n",
    "        #                         START OF YOUR CODE                               #\n",
    "        ############################################################################\n",
    "        \n",
    "        \n",
    "\n",
    "        ############################################################################\n",
    "        #                          END OF YOUR CODE                                #\n",
    "        ############################################################################\n",
    "\n",
    "        # for classification, normalize the last dimension\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        # for regression, normalize each entry\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, transforms):\n",
    "        '''\n",
    "        Args\n",
    "        ----\n",
    "        - x -> Tensor: the input image\n",
    "        - transforms -> list: a list of transforms giving different input scales\n",
    "        '''\n",
    "\n",
    "        cls, bbs = [], []\n",
    "\n",
    "        for t in transforms:\n",
    "            # feature extraction\n",
    "            feat = self.featurizer(t(x))\n",
    "            # build the query\n",
    "            q = self.query.weight.repeat(x.shape[0], 1, 1)\n",
    "            # apply transformer\n",
    "            z = self.transformer(q, feat)\n",
    "            # class prediction\n",
    "            cls.append(self.softmax(self.classifier(z)))\n",
    "            # bounding box prediction\n",
    "            bbs.append(self.sigmoid(self.regressor(z)))\n",
    "\n",
    "        cls = torch.concat(cls, dim=1)\n",
    "        bbs = torch.concat(bbs, dim=1)\n",
    "\n",
    "        return cls, bbs\n",
    "\n",
    "\n",
    "# unit test\n",
    "model = DETR(256, 2, 8, 2, resnet18, ResNet18_Weights.DEFAULT, 7)\n",
    "# we may specify various different input scales and ratios\n",
    "t1 = transforms.Compose([\n",
    "    transforms.Resize((380, 380), antialias=True),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "t2 = transforms.Compose([\n",
    "    transforms.Resize((480, 380), antialias=True),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "# feedforward\n",
    "conf, boxes = model(Xt.unsqueeze(0), [t1, t2])\n",
    "# there will be 2(transforms)x8(#quries) predictions\n",
    "conf.shape, boxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMTAZg6SDm1X"
   },
   "source": [
    "## 3 - Loss Calculation\n",
    "\n",
    "A model by itself wouldn't accomplish anything without appropriate training, and loss calculation plays quite a deterministic role in this process. Like most modern object detectors, `DETR` adopts a compound loss leveraging between multiple factors.\n",
    "\n",
    "For each input image with objects from various classes appearing in various locations, the model proposed to generate a fixed number of predictions ($M$, in our cases, detremined by the number of queries) for both the class scores and bounding box coordinates. In order to evalute the quality of these predictions, we consider two major aspects:\n",
    "\n",
    "- <font color=\"red\">***Classification Loss $L_{cls}$:***</font> the quality of class scores for each prediction, i.e. which of the predictions should be considered objects and which be the background\n",
    "\n",
    "- ***Localization Loss:*** the quality of the bounding box coordinates, i.e. whether they *correctly* covers the object, which can be measured by two following metrics:\n",
    "    - <font color=\"red\">***Distance Loss $L_{dst}$:***</font> how far the predicted boxes are from the ground truths\n",
    "    - <font color=\"red\">***Box Loss $L_{bbs}$:***</font> how well the predicted boxes overlaps the ground truths\n",
    "\n",
    "For some predictions $\\hat{y} = [\\hat{y_1}, \\dots, \\hat{y_M}]$ and ground truths $y = [y_1, \\dots, y_M]$,\n",
    "\n",
    "$$\n",
    "L = \\alpha L_{cls} (\\hat{y}, y) + \\beta L_{dst} (\\hat{y}, y) + \\gamma L_{bbs} (\\hat{y}, y)\n",
    "$$\n",
    "\n",
    "Apparently, we need **one-to-one** correspondence between $\\hat{y_i}$ and $y_i$ to calculate the loss. This bipartite matching problem is proposed to be addressed by the Hungarian algorithm with a compound cost matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnFa_Squ0iqg"
   },
   "source": [
    "Here we implement the `DETRLoss`.\n",
    "\n",
    "***Note:***\n",
    "- Always refer to the original code base\n",
    "- For each prediction in `cls_preds`, we mannually construct the corresponding `cls_target` based on bipartite matching results (whether this prediction matches a ground truth better than any other predictions)\n",
    "- We use all predicted scores to calculate the classification loss\n",
    "- We use only the selected (matched) boxes to calculate localization losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1171,
     "status": "ok",
     "timestamp": 1679935733929,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "LBhlSY-uDmfO",
    "outputId": "03f49b2d-14bb-4493-a8e8-585ac50c02bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.ops.boxes import generalized_box_iou\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "class DETRLoss(nn.Module):\n",
    "    '''\n",
    "    DETR Loss Calculation\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    cls_loss_coef: classification loss coefficient\n",
    "    dst_loss_coef: distance loss coefficient\n",
    "    bbs_loss_coef: bounding-box loss coefficient\n",
    "    cls_bg_wt: classification background class weight\n",
    "    '''\n",
    "\n",
    "    def __init__(self, cls_loss_coef=1, dst_loss_coef=1, bbs_loss_coef=1, cls_bg_wt=0.5):\n",
    "        super().__init__()\n",
    "        self.cls_loss_coef = cls_loss_coef\n",
    "        self.dst_loss_coef = dst_loss_coef\n",
    "        self.bbs_loss_coef = bbs_loss_coef\n",
    "\n",
    "        weight = torch.Tensor([cls_bg_wt, 1 - cls_bg_wt])\n",
    "        self.cls_loss_func = nn.CrossEntropyLoss(weight)\n",
    "        self.dst_loss_func = nn.L1Loss()\n",
    "        self.bbs_loss_func = generalized_box_iou#generalized_box_iou_loss\n",
    "\n",
    "        self.hists = {}\n",
    "\n",
    "    def update(self, **losses):\n",
    "        ''' Store the calculated loss values for logging purpose '''\n",
    "        for k, v in losses.items():\n",
    "            self.hists.setdefault(k, []).append(v.item())\n",
    "\n",
    "    def summary(self):\n",
    "        ''' Summarize the logged info and clear history '''\n",
    "        s = {k: np.mean(v) for k, v in self.hists.items()}\n",
    "        self.hists = {}\n",
    "        return s\n",
    "\n",
    "    def match(self, cls_preds, bbs_preds, cls_target, bbs_target):\n",
    "        '''\n",
    "        Match predictions with targets\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        idx_preds: indices of selected predictions\n",
    "        idx_target: indices of corresponding targets\n",
    "        '''\n",
    "\n",
    "        # build the cost matrix\n",
    "        cls_cost = 1 - cls_preds[:, cls_target]\n",
    "        dst_cost = torch.cdist(bbs_preds, bbs_target, p=1)\n",
    "        box_cost = -generalized_box_iou(bbs_preds, bbs_target)\n",
    "        costs = self.cls_loss_coef * cls_cost + self.dst_loss_coef * dst_cost + self.bbs_loss_coef * box_cost\n",
    "\n",
    "        # hungarian algorithem\n",
    "        idx_preds, idx_target = linear_sum_assignment(costs)\n",
    "\n",
    "        return idx_preds, idx_target\n",
    "\n",
    "    def forward(self, cls_preds, bbs_preds, cls_target, bbs_target):\n",
    "        '''\n",
    "        Calculate the losses\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        cls_preds: predicted class scores of shape [batch_size, M, 2]\n",
    "        bbs_preds: predicted bounding boxes of shape [batch_size, M, 4]\n",
    "        cls_target: ground truth class scores of [batch_size, N]\n",
    "        bbs_target: ground truth bounding boxes of shape [batch_size, N, 4]\n",
    "        '''\n",
    "\n",
    "        # flatten the batches\n",
    "        cls_preds = cls_preds.flatten(0, -2)\n",
    "        bbs_preds = bbs_preds.flatten(0, -2)\n",
    "        cls_target = cls_target.flatten(0, -1)\n",
    "        bbs_target = bbs_target.flatten(0, -2)\n",
    "\n",
    "        # match preds with target\n",
    "        idx_preds, idx_target = self.match(\n",
    "            cls_preds.detach().cpu(),\n",
    "            bbs_preds.detach().cpu(),\n",
    "            cls_target.detach().cpu(),\n",
    "            bbs_target.detach().cpu()\n",
    "        )\n",
    "\n",
    "        # construct class prediction and target with background\n",
    "        cls_target = torch.zeros(cls_preds.shape[0], dtype=cls_target.dtype, device=cls_target.device)\n",
    "        cls_target[idx_preds] = 1\n",
    "\n",
    "        cls_loss = self.cls_loss_func(cls_preds, cls_target) * self.cls_loss_coef\n",
    "        dst_loss = self.dst_loss_func(bbs_preds[idx_preds], bbs_target[idx_target]) * self.dst_loss_coef\n",
    "        bbs_loss = 1 - torch.diag(self.bbs_loss_func(bbs_preds[idx_preds], bbs_target[idx_target]))\n",
    "        bbs_loss = bbs_loss.sum() / len(bbs_target) * self.bbs_loss_coef\n",
    "\n",
    "        # update the logs for every calculation\n",
    "        self.update(cls_loss=cls_loss, dst_loss=dst_loss, bbs_loss=bbs_loss)\n",
    "\n",
    "        return cls_loss + dst_loss + bbs_loss\n",
    "\n",
    "\n",
    "# unit test\n",
    "loss = DETRLoss(1, 1, 1, 0.5)\n",
    "l = loss(conf.squeeze(0), boxes.squeeze(0), yt['labels'], yt['boxes'])\n",
    "print(l, loss.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7X4cDoz0Ufg"
   },
   "source": [
    "## 4 - Training\n",
    "\n",
    "Now we can train our `DETR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, train_set, test_set, loss, optim, transforms, device):\n",
    "    '''\n",
    "    Trainer\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    - model: the DETR model\n",
    "    - train_set/test_set: train/test datasets\n",
    "\n",
    "    - loss: loss function\n",
    "    - optim: optimizer\n",
    "    - transforms -> list: a list of different image transforms (with different scales and ratios)\n",
    "\n",
    "    - device: CPU or GPU\n",
    "\n",
    "    Return\n",
    "    ----\n",
    "    - hists -> dict: training logs\n",
    "    '''\n",
    "\n",
    "    # dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=1, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, num_workers=2)\n",
    "\n",
    "    # logs\n",
    "    hists = {'train': [], 'test': []}\n",
    "\n",
    "    # send to device\n",
    "    model = model.to(device)\n",
    "    loss = loss.to(device)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print('Epoch {}/{}:'.format(e + 1, epochs))\n",
    "\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            cls, bbs = model(X.to(device), transforms)\n",
    "            l = loss(cls, bbs, y['labels'].to(device), y['boxes'].to(device))\n",
    "            optim.zero_grad()\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            train_loss += l.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        losses = loss.summary()\n",
    "        print('\\t train loss {:.4f} ({})'.format(\n",
    "            train_loss, ', '.join(['{} {:.4f}'.format(k, v) for k, v in losses.items()])\n",
    "        ))\n",
    "        hists['train'].append([train_loss, *list(losses.values())])\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (X, y) in enumerate(test_loader):\n",
    "                cls, bbs = model(X.to(device), ts)\n",
    "                l = loss(cls, bbs, y['labels'].to(device), y['boxes'].to(device))\n",
    "                test_loss += l.item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        losses = loss.summary()\n",
    "        print('\\t test  loss {:.4f} ({})'.format(\n",
    "            test_loss, ', '.join(['{} {:.4f}'.format(k, v) for k, v in losses.items()])\n",
    "        ))\n",
    "        hists['test'].append([test_loss, *list(losses.values())])\n",
    "\n",
    "    return hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_hists(hists):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    # train\n",
    "    lines = ax[0].plot(np.array(hists['train']))\n",
    "    ax[0].legend(lines, ['total_loss', 'cls_loss', 'dst_loss', 'bbs_loss'])\n",
    "    ax[0].set_xlabel('Epoches')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Train')\n",
    "    # test\n",
    "    lines = ax[1].plot(np.array(hists['test']))\n",
    "    ax[1].legend(lines, ['total_loss', 'cls_loss', 'dst_loss', 'bbs_loss'])\n",
    "    ax[1].set_xlabel('Epoches')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].set_title('Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB1pYGZgenzo"
   },
   "source": [
    "<font color=\"red\">***TODO:***</font> Complete the training setup and make the loss decrease:\n",
    "- Configure the model architecture\n",
    "- Configure the coefficients for `DETRLoss` (**IMPORTANT**)\n",
    "- Choose an optimizer for `optim`\n",
    "- Choose an input scale for `scale`\n",
    "\n",
    "***NOTE:*** It is a non-trivial effort to set good loss coefficients, and small deviation in their values may completely mess up the training. No optimals exist in general because they are dependent on both your model configuration and your dataset. However, [the original code base](https://github.com/facebookresearch/detr/blob/main/main.py) provides a good starting point from which you could fine tune.\n",
    "\n",
    "***HINT:***\n",
    "- You **MUST** properly balance the classification loss, as there are way too many backgrounds and too few pedestrians\n",
    "- Try to get a `cls` matrix with all values close to `0.3 - 0.7` at the beginning of your training (e.g. after the 1st epoch)\n",
    "- Don't focus on the compound loss value, **EACH** of the three loss terms requires an observable decrease for the model to function (large nagative loss values might indicate an unfortunate initialization or poor choice of coefficients, try to start over)\n",
    "- Learn to observe the intermediate variables and visualized results in order to identify a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5240,
     "status": "ok",
     "timestamp": 1679935749336,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "mAQCEDxr09yg",
    "outputId": "f31060f6-10ec-4501-cfa2-ca2b0d8c6ffb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "model = \n",
    "\n",
    "epochs = \n",
    "loss = \n",
    "optim = \n",
    "\n",
    "scales = [(380, 380), (480, 480)] # TODO: change this if needed\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "ts = [\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(s, antialias=True),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    for s in scales\n",
    "]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "hists = train(model, epochs, train_set, test_set, loss, optim, ts, device)\n",
    "plot_hists(hists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">***TODO:***</font> Experiment with ***AT LEAST 3*** different sets of hyperparameters (e.g. model configuration, loss coefficients, learning rate and scales). _Briefly_ state your findings for each experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Experiment 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">***Answer:***</font> [Your findings here]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Experiment 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">***Answer:***</font> [Your findings here]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Experiment 3***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#                         START OF YOUR CODE                               #\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "############################################################################\n",
    "#                          END OF YOUR CODE                                #\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">***Answer:***</font> [Your findings here]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4zlS8z2l9lL"
   },
   "source": [
    "## 5 - Average Precision\n",
    "\n",
    "[Average Precision (AP)](https://www.v7labs.com/blog/mean-average-precision) is the most important metric to look for when evaluating object detection models. This (as well as other criterions) will be covered in future lectures.\n",
    "\n",
    "First, get a sample to develop our algorithms.\n",
    "\n",
    "***Note:*** We set a threshold of `cls > 0.5` for the boxes to display. The AP, however, is calculated on all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "executionInfo": {
     "elapsed": 2098,
     "status": "ok",
     "timestamp": 1679935954433,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "n8QqSKi9MZdk",
    "outputId": "bb469227-170a-4101-bc2f-58782428cd73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "idx = np.random.choice(len(test_set))\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "Xt, yt = test_set[idx]\n",
    "cls, bbs = model(Xt.to(device).unsqueeze(0), ts)\n",
    "cls = cls.detach().view(-1, 2).cpu()[:, 1]\n",
    "bbs = bbs.detach().view(-1, 4).cpu()\n",
    "idx_preds = torch.where(cls > 0.5)\n",
    "\n",
    "img = draw_boxes(\n",
    "    Xt, bbs[idx_preds], cls[idx_preds],\n",
    "    [PENNFUDAN_LABEL_NAMES[1] for _ in range(len(idx_preds[0]))],\n",
    "    color=(1., 0., 0)\n",
    ")\n",
    "img = draw_boxes(img, yt['boxes'], color=(0, 1., 0))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMzwCLH1p7Gy"
   },
   "source": [
    "Next, let's implement the `average_precision`. Precision and recall have already been calculated for you, just\n",
    "- Smooth out the `precision` into `precision_smooth` (refer to **Lecture 1&2** if you don't know what this means)\n",
    "- Calculate the average precision and justify it by comparing the result from `torchmetrics`\n",
    "\n",
    "**Reference:** [rafaelpadilla/Object-Detection-Metrics](https://github.com/rafaelpadilla/Object-Detection-Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1679935957631,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "Ufzw9vPrc8_-",
    "outputId": "c6f1572a-6ecd-4ec8-a6ea-87bcf7f94f4e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.ops.boxes import box_iou\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "\n",
    "def average_precision_torch(cls_preds, bbs_preds, bbs_target, iou_thresh=0.5):\n",
    "    # MAP with torchmetrics\n",
    "    MAP = MeanAveragePrecision(iou_thresholds=[iou_thresh])\n",
    "    MAP.update(\n",
    "        preds=[{\n",
    "            'boxes': bbs_preds,\n",
    "            'scores': cls_preds,\n",
    "            'labels': torch.ones(len(bbs_preds), dtype=int)\n",
    "        }],\n",
    "        target=[{\n",
    "            'boxes': bbs_target,\n",
    "            'labels': torch.ones(len(bbs_target), dtype=int)\n",
    "        }]\n",
    "    )\n",
    "    return MAP.compute()['map'].item()\n",
    "\n",
    "\n",
    "def average_precision(cls_preds, bbs_preds, bbs_target, iou_thresh=0.5, ax=None):\n",
    "    '''\n",
    "    Average Precision (single image)\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    cls_preds: predicted class scores\n",
    "    bbs_preds: predicted bounding boxes\n",
    "    bbs_target: ground truth bounding boxes\n",
    "    iou_thresh: IOU threshold for TP/FP criterion\n",
    "    ax: plot PR-curve\n",
    "    '''\n",
    "\n",
    "    # sort the predictions by confidence score\n",
    "    idx = torch.argsort(cls_preds, descending=True)\n",
    "    bbs_preds = bbs_preds[idx]\n",
    "\n",
    "    # calculate IOUs\n",
    "    ious = box_iou(bbs_preds, bbs_target)\n",
    "\n",
    "    # check if each prediction is TP\n",
    "    tps = np.zeros(bbs_preds.shape[0])\n",
    "    target_idx = list(range(len(bbs_target)))\n",
    "    for p in range(len(bbs_preds)):\n",
    "        iou_max = 0\n",
    "        iou_max_id = -1\n",
    "        for t in target_idx:\n",
    "            if ious[p, t] > iou_thresh:\n",
    "                if ious[p, t] > iou_max:\n",
    "                    iou_max = ious[p, t]\n",
    "                    iou_max_id = t\n",
    "        if iou_max_id != -1:\n",
    "            tps[p] = 1\n",
    "            target_idx.remove(iou_max_id)\n",
    "\n",
    "    # whatever is not TP, it's an FP\n",
    "    fps = 1 - tps\n",
    "\n",
    "    # calculate P/R\n",
    "    tps_acc = tps.cumsum()\n",
    "    fps_acc = fps.cumsum()\n",
    "    precision = tps_acc / (tps_acc + fps_acc)\n",
    "    recall = tps_acc / len(bbs_target)\n",
    "    precision = np.concatenate([[1], precision, [0]])\n",
    "    recall = np.concatenate([[0], recall, [1]])\n",
    "\n",
    "    # smooth out the precision\n",
    "    precision_smooth = np.zeros_like(precision)\n",
    "    for i in range(len(precision) - 1, 0, -1):\n",
    "        precision_smooth[i - 1] = max(precision_smooth[i], precision[i - 1])\n",
    "\n",
    "    # calculate AP\n",
    "    ap = np.trapz(precision_smooth, recall)\n",
    "\n",
    "    # create plot\n",
    "    if ax is not None:\n",
    "        ax.plot(recall, precision)\n",
    "        ax.plot(recall, precision_smooth)\n",
    "\n",
    "    return ap\n",
    "\n",
    "\n",
    "# unit test\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "plt.grid('on')\n",
    "plt.title('Average Precition')\n",
    "plt.xlabel('precition')\n",
    "plt.ylabel('recall')\n",
    "\n",
    "ap = average_precision(cls, bbs, yt['boxes'], 0.5, ax=ax)\n",
    "ap_torch = average_precision_torch(cls, bbs, yt['boxes'], 0.5)\n",
    "print(ap, ap_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMi7y2vOu7hr"
   },
   "source": [
    "## 6 - Results\n",
    "\n",
    "Let's see the results on the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 2747,
     "status": "ok",
     "timestamp": 1679935964270,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "VJGzS6YIwUQr",
    "outputId": "f7a4333a-cefa-4796-cb20-03017904f964",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AP\n",
    "aps, aps_torch = [], []\n",
    "iou_thresh = 0.5\n",
    "with torch.no_grad():\n",
    "    for i, (X, y) in enumerate(test_set):\n",
    "        cls, bbs = model(X.unsqueeze(0).to(device), ts)\n",
    "        cls = cls.detach().view(-1, 2).cpu()[:, 1]\n",
    "        bbs = bbs.detach().view(-1, 4).cpu()\n",
    "        tgt = y['boxes']\n",
    "        aps.append(average_precision(cls, bbs, tgt, iou_thresh))\n",
    "        aps_torch.append(average_precision_torch(cls, bbs, tgt, iou_thresh))\n",
    "\n",
    "# two curves should be largely overlapping\n",
    "plt.plot(aps)\n",
    "plt.plot(aps_torch)\n",
    "plt.legend(['AP', 'AP-torch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRwCu0LkWCpa"
   },
   "source": [
    "For comparison, let's load an authentic pretrained `DETR` and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201,
     "referenced_widgets": [
      "2ec936d23b3e49cbb38bdc1d75749e6b",
      "500a8ad2ba3e4bf0ab2fade550166a3b",
      "d4e82c4fb19f473a805f4a2603ce8403",
      "bbbd476969224dc6b288801717bc3a37",
      "046b66792d614528be4c04c999c4e9bd",
      "a1b0e431f21b49a2885e9dbd39d32f60",
      "f82d3e4ef4df40998c18d1bb8b657bf5",
      "b1d4c4489da54385bcb7d25ebdef9a4a",
      "36fc7b8f14374f0f900b0d9637dc1108",
      "1ca3cbb9bd9b45dda9c725bb1c2f4642",
      "d79e9bb7ecbf45e481df0103f446fc9e",
      "581e4d41bcac4623840c4bf4902ebe8a",
      "68e21d8e89e94c488c2b6f4e8764e06a",
      "167b612e78274384afbf5437a0244c5f",
      "76b42e32e1e742d98dd6130702e4e771",
      "1e7e9347c91c44e9a1d2c6ec16cd8689",
      "cee57e2d7d1a4f428738cf2b5f87df66",
      "04d6774a411840718e9f18480671a194",
      "67c9e644b502431592319161c799e3f6",
      "b31b799c77314b229dc10ac7b956c6c6",
      "e656d84216ce4b698b13e73d024aa586",
      "273009a284d74df69aaeb33a75aef9e4",
      "945c04aac89c43c59d765a425edba9c1",
      "ce72f74b09534bb79fcbad143b487a7b",
      "39a95df279974e828591944c04929239",
      "b90109fd18854933828a75fff92089eb",
      "aecba2e612b3496ebde80add97b11bcf",
      "047ef40428954d0999957418bc9c8c3d",
      "779eae05a27c42e4938623717a239546",
      "5b52625e436e4c00a1dcd1e248b5d0bb",
      "8e2431d662ec461099efb657efc7bd5e",
      "fe3718d9dad34988b41699afdbe23e6a",
      "95c1ee0c2347419a85b9a1e7aaa14065"
     ]
    },
    "executionInfo": {
     "elapsed": 16481,
     "status": "ok",
     "timestamp": 1679935986394,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "AUosTyelW0E8",
    "outputId": "acb6711e-156c-48ca-8079-48a78130b78e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, DetrForObjectDetection\n",
    "\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/detr-resnet-50')\n",
    "detr = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\n",
    "\n",
    "img = Image.open('PennFudanPed/PNGImages/{}'.format(test_set.dataset.imgs[yt['image_id'].item()]))\n",
    "inputs = processor(images=img, return_tensors='pt')\n",
    "outputs = detr(**inputs)\n",
    "\n",
    "results = processor.post_process_object_detection(outputs, threshold=0.9, target_sizes=torch.tensor([(1, 1)]))[0]\n",
    "average_precision_torch(results['scores'], results['boxes'], yt['boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As3wseLvqYZU"
   },
   "source": [
    "Finally, let's visualize some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "executionInfo": {
     "elapsed": 15866,
     "status": "ok",
     "timestamp": 1679936006917,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "e7scfnWn2G1-",
    "outputId": "248209b7-ca55-4ce4-c7a4-3add46c01934",
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(test_set), 5, replace=False)\n",
    "fig, ax = plt.subplots(2, len(idx), figsize=(20, 10))\n",
    "conf_thresh = 0.9\n",
    "\n",
    "with torch.no_grad():\n",
    "    for ii, i in enumerate(idx):\n",
    "        Xt, yt = test_set[i]\n",
    "\n",
    "        # our detr\n",
    "        cls, bbs = model(Xt.to(device).unsqueeze(0), ts)\n",
    "        cls = cls.detach().view(-1, 2).cpu()[:, 1]\n",
    "        bbs = bbs.detach().view(-1, 4).cpu()\n",
    "        idx_preds = torch.where(cls > conf_thresh)\n",
    "        img = draw_boxes(\n",
    "            Xt, bbs[idx_preds], cls[idx_preds],\n",
    "            [PENNFUDAN_LABEL_NAMES[1] for _ in range(len(idx_preds[0]))],\n",
    "            color=(1., 0, 0)\n",
    "        )\n",
    "        img = draw_boxes(img, yt['boxes'], color=(0, 1., 0))\n",
    "        ax[0][ii].imshow(img)\n",
    "        ax[0][ii].set_title('AP = {:.4f}'.format(average_precision(cls[idx_preds], bbs[idx_preds], yt['boxes'])))\n",
    "        ax[0][ii].axis('off')\n",
    "\n",
    "        # pretrained detr\n",
    "        img = Image.open('PennFudanPed/PNGImages/{}'.format(test_set.dataset.imgs[yt['image_id'].item()]))\n",
    "        inputs = processor(images=img, return_tensors=\"pt\")\n",
    "        outputs = detr(**inputs)\n",
    "        results = processor.post_process_object_detection(outputs, target_sizes=[(1, 1)], threshold=conf_thresh)[0]\n",
    "        img = draw_boxes(\n",
    "            Xt, results['boxes'], results['scores'],\n",
    "            [COCO_LABEL_NAMES[j] for j in results['labels']],\n",
    "            color=(1., 0, 0)\n",
    "        )\n",
    "        img = draw_boxes(img, yt['boxes'], color=(0, 1., 0))\n",
    "        ax[1][ii].imshow(img)\n",
    "        ax[1][ii].set_title('AP = {:.4f}'.format(average_precision(results['scores'], results['boxes'], yt['boxes'])))\n",
    "        ax[1][ii].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKZw5en9WPMt"
   },
   "source": [
    "## 7 - Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKgMsvn4onbG"
   },
   "source": [
    "### 7.1 - Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the core idea of Transformers, attention mechanism is formulated as follows.\n",
    "\n",
    "Let's consider some value (data sample) $v_i \\in R^D$. We pair it up with a key $k_i \\in R^d$ and store this key-value pair $(k_i, v_i)$ into some \"database\". There are $N$ entries of these key-value pairs in total, and we represent them as\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "K = [k_1, \\dots, k_N]^T \\in R^{N \\times d} \\\\\n",
    "V = [v_1, \\dots, v_N]^T \\in R^{N \\times D} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We are interested in retrieving from this database some desired value using a query $q_j \\in R^d$. To do this, we need to know how the keys in our database resemble the query $q_j$ (note that the queries $q_j$ must have the same size with the keys $k_i$, but not necessarily the values $v_i \\in R^D$).\n",
    "\n",
    "For a particular key $k_i$, the resemblence score can be measured by their dot-product, i.e.\n",
    "\n",
    "$$\n",
    "s_{j,i}' := q_j^T k_i \\in R\n",
    "$$\n",
    "\n",
    "We call this resemblence score ***attention***. Then for all the keys in our database, their attention w.r.t $q_j$ is\n",
    "\n",
    "$$\n",
    "A_j' = [q_j^T k_1, \\dots, q_j^T k_N] = [s_{j,1}', \\dots, s_{j,N}'] = q_j^T K^T \\in R^N\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bms9cKJMWVUX"
   },
   "source": [
    "Since the value of the dot-product $s_{j,i}'$ is likely to be affected by its own dimensionality (more entries would simply give larger results), the attention score is first scaled by dividing the square root of the dimension $d$, and then normalized so that they sum up to $1$.\n",
    "\n",
    "Formally, write\n",
    "\n",
    "$$\n",
    "s_{j,i} := \\sigma \\left( \\frac{s_{j,i}'}{\\sqrt{d}} \\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma: R^N \\to [0,1]^N$ is the `softmax` function. A briefer notation reads\n",
    "\n",
    "$$\n",
    "A_j = [s_{j,1}, \\dots, s_{j,N}] = \\sigma \\left( \\frac{A_j'}{\\sqrt{d}} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the retrieved value as a linear combinition of all stored values in our database ***weighted by the normalized attention scores***\n",
    "\n",
    "$$\n",
    "O_j = \\sum_{i=1}^N s_{j,i} v_i = A_j V \\in R^D\n",
    "$$\n",
    "\n",
    "Then if we have $M$ queries arranged as $Q = [q_1, \\dots, q_M]^T \\in R^{M \\times d}$, the attentions are\n",
    "\n",
    "$$\n",
    "A = [A_1, \\dots, A_M]^T = \\sigma (\\frac{Q K^T}{\\sqrt{d}}) \\in R^{M \\times N}\n",
    "$$\n",
    "\n",
    "where `softmax` is computed along the **last**Now we compute the retrieved value as a linear combinition of all stored values in our database ***weighted by the normalized attention scores***\n",
    "\n",
    "$$\n",
    "O_j = \\sum_{i=1}^N s_{j,i} v_i = A_j V \\in R^D\n",
    "$$\n",
    "\n",
    "Then if we have $M$ queries arranged as $Q = [q_1, \\dots, q_M]^T \\in R^{M \\times d}$, the attentions are\n",
    "\n",
    "$$\n",
    "A = [A_1, \\dots, A_M]^T = \\sigma (\\frac{Q K^T}{\\sqrt{d}}) \\in R^{M \\times N}\n",
    "$$\n",
    "\n",
    "where `softmax` is computed along the last dimension. And the output is\n",
    "\n",
    "$$\n",
    "O = [O_1, \\dots, O_M] = [A_1 V, \\dots, A_M V]^T = A V \\in R^{M \\times D}\n",
    "$$\n",
    "\n",
    "Therefore, the attention function is formulated as\n",
    "\n",
    "$$\n",
    "\\text{Att } (Q, K, V) := \\sigma (\\frac{Q K^T}{\\sqrt{d}}) V\n",
    "$$ dimension. And the output is\n",
    "\n",
    "$$\n",
    "O = [O_1, \\dots, O_M] = [A_1 V, \\dots, A_M V]^T = A V \\in R^{M \\times D}\n",
    "$$\n",
    "\n",
    "Therefore, the attention function is formulated as\n",
    "\n",
    "$$\n",
    "\\text{Att } (Q, K, V) := \\sigma (\\frac{Q K^T}{\\sqrt{d}}) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MDrA1xKoRXC"
   },
   "source": [
    "In order to make full use of the architecture of a GPU, the above computation can be further parallelized by decomposing the feature dimension into $h$ **attention heads**.\n",
    "\n",
    "Let's start by the following reshaping:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "Q : \\quad R^{M \\times d} \\to R^{M \\times (h \\cdot \\frac{d}{h})} \\to R^{h \\times M \\times d'} \\\\\n",
    "K : \\quad R^{N \\times d} \\to R^{N \\times (h \\cdot \\frac{d}{h})} \\to R^{h \\times N \\times d'} \\\\\n",
    "V : \\quad R^{N \\times D} \\to R^{N \\times (h \\cdot \\frac{D}{h})} \\to R^{h \\times N \\times D'} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $d' := d/h$ and $D' := D/h$.\n",
    "\n",
    "For each head, we have\n",
    "\n",
    "$$\n",
    "i \\in \\{1, \\dots, h\\} \\quad\n",
    "\\begin{cases}\n",
    "Q_i \\in R^{M \\times d'} \\\\\n",
    "K_i \\in R^{N \\times d'} \\\\\n",
    "V_i \\in R^{N \\times D'} \\\\\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the attention operation can be easily propagated by:\n",
    "1. compute the attention individually on each head (this is fully parallelizable),\n",
    "2. gather the results from each attention head and concatenate them together, i.e.\n",
    "\n",
    "$$\n",
    "O = [\\text{Att } (Q_1, K_1, V_1), \\dots, \\text{Att } (Q_h, K_h, V_h)] \\in R^{h \\times M \\times D'}\n",
    "$$\n",
    "\n",
    "The output is finally reshaped back to\n",
    "\n",
    "$$\n",
    "O : \\quad R^{h \\times M \\times D'} \\to R^{M \\times (h \\cdot D')} \\to R^{M \\times D}\n",
    "$$\n",
    "\n",
    "For simplicity, denote the above operations (decompose, multiply and concatenate) as $\\otimes_h: R^{M \\times d} \\times R^{d \\times N} \\to R^{M \\times N}$,\n",
    "\n",
    "$$\n",
    "Q \\otimes_h K^T = [Q_1 K_1^T, \\dots, Q_h K_h^T], \\quad \\text{where} \\quad\n",
    "\\begin{cases} Q_i \\in R^{M \\times d'} \\\\ K_i \\in R^{M \\times D'} \\end{cases}\n",
    "$$\n",
    "\n",
    "The multi-head attention function can be written as\n",
    "\n",
    "$$\n",
    "\\text{MHAtt } (Q, K, V, h) := \\sigma (\\frac{Q \\otimes_h K^T}{\\sqrt{d/h}}) \\otimes_h V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ig6lBFEajtxI"
   },
   "source": [
    "### 7.3 Permutational Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8ttm1qPr9rl"
   },
   "source": [
    "It is quite intuitive to see why attention functions are permutation invariant. \n",
    "\n",
    "Consider some key-value pairs $k_i \\in R^d, v_i \\in R^D$ originally indexed by $I = \\{i_1, \\dots, i_N\\}$ and a particular query $q \\in R^d$.\n",
    "\n",
    "The attention output reads\n",
    "\n",
    "$$\n",
    "o = \\sum_{i \\in I} s_i v_i = \\sum_{n = 1}^N s_{i_n} v_{i_n} \\in R^D,\n",
    "\\quad \\text{where } \\quad s_{i_n} = \\sigma (\\frac{q^T k_{i_n}}{\\sqrt{d}})\n",
    "$$\n",
    "\n",
    "Now we consider a permutation of $I$ denoted $J = \\{j_1, \\dots, j_N\\}$ containing the shuffled indices in $I$, and we can use $J$ to reindex the key-value pairs (i.e. shuffle the order of our keys and values). Then the new output with the same query $q$ is computed as\n",
    "\n",
    "$$\n",
    "o' = \\sum_{j \\in J} s_j v_j = \\sum_{n = 1}^N s_{j_n} v_{j_n}\n",
    "$$\n",
    "\n",
    "Since $J$ contains *exactly* the same components as $I$ (only shuffled), there exists a specific order of $n_1, \\dots, n_N$ such that\n",
    "\n",
    "$$\n",
    "i_1, \\dots, i_N = j_{n_1}, \\dots, j_{n_N}\n",
    "$$\n",
    "\n",
    "Therefore a summation over $J$ is equivalant to a summation over $I$ (exchange rule). It follows that\n",
    "\n",
    "$$\n",
    "o' = \\sum_{n = 1}^N s_{j_n} v_{j_n} = \\sum_{n = 1}^N s_{i_n} v_{i_n} = o\n",
    "$$\n",
    "\n",
    "which indicates the order in that the key-values pairs are arranged has no effect on the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIZovLv5bY2_"
   },
   "source": [
    "Let's show it with our test example from **Section 2.3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1679936357715,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "BzFAjDqXbdYi",
    "outputId": "912e7814-0e55-4027-f0f8-2a664afbd607",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct q & kv\n",
    "# you need to run through section 2 before excecuting this cell\n",
    "q = torch.randn((1, 4, dim)).double()\n",
    "kv = feat.flatten(2, 3).permute(0, 2, 1).double()\n",
    "dim = kv.shape[-1]\n",
    "\n",
    "# permute it\n",
    "I = np.arange(kv.shape[1])\n",
    "J = I.copy()\n",
    "np.random.shuffle(J)\n",
    "kv_permuted = kv[:, J, :]\n",
    "\n",
    "# test (use dtype=double to avoid numeric errors)\n",
    "attention = MultiHeadAttention(dim, 8, 0).double()\n",
    "att = attention(q, kv, kv).detach()\n",
    "att_permuted = attention(q, kv_permuted, kv_permuted).detach()\n",
    "\n",
    "# are they equal?\n",
    "assert torch.allclose(att_permuted, att)\n",
    "print('Absolute error:', torch.norm(att_permuted - att, p=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmFFtNGBxW_u"
   },
   "source": [
    "This makes no difference if the order of the key-value pairs is negligible, which, more often than not, is unsatisfactory (e.g. in detection tasks, each position corresponds to a specific spatial location in the original image and cannot be exchanged). This is proposed to be addressed by introducing positional encodings.\n",
    "\n",
    "A positional encoding, by the essence, can be thought of as a function $P: R^d \\to R^d$ that generates particular features solely dependent on the position $n$, i.e. the positional encoding of some key $k_{i_n}$ (or $k_{j_n}$ likewise) writes\n",
    "\n",
    "$$\n",
    "p_n := P(k_{i_n}) = P(k_{j_n}) \\in R^d\n",
    "$$\n",
    "\n",
    "Consider a set of positional encodings $p_1, \\dots, p_N$ corresponding to the the keys $k_{i_1}, \\dots, k_{i_N}$. The positionally-encoded attention scores $\\tilde{s}_i$ can be decomposed into a *feature* score and a *position* score, i.e.\n",
    "\n",
    "$$\n",
    "\\tilde{s}_{i_n} = q^T (k_{i_n} + p_n) = s_i + q^T p_n\n",
    "$$\n",
    "\n",
    "Normalization with `softmax` is ignored for simplicity. Then the output writes\n",
    "\n",
    "$$\n",
    "\\tilde{o} = \\sum_{i \\in I} \\tilde{s}_i v_i = \\sum_{n = 1}^N s_{i_n} v_{i_n} + \\sum_{n = 1} q^T p_n v_{i_n}\n",
    "$$\n",
    "\n",
    "After a permutation with $J$, the output becomes\n",
    "\n",
    "$$\n",
    "\\tilde{o}' = \\sum_{j \\in J} \\tilde{s}_j v_j = \\sum_{n = 1}^N s_{j_n} v_{j_n} + \\sum_{n = 1} q^T p_n v_{j_n}\n",
    "$$\n",
    "\n",
    "Since we have shown that for the first term in $\\tilde{o}'$ and $\\tilde{o}$\n",
    "\n",
    "$$\n",
    "\\sum_{n = 1}^N s_{j_n} v_{j_n} = \\sum_{n = 1}^N s_{i_n} v_{i_n}\n",
    "$$\n",
    "\n",
    "and obviously for the second term\n",
    "\n",
    "$$\n",
    "\\sum_{n = 1}^N q^T p_n \\color{red}{v_{j_n}} \\ne \\sum_{n = 1}^N q^T p_n \\color{red}{v_{i_n}}\n",
    "$$\n",
    "\n",
    "It follows that\n",
    "\n",
    "$$\n",
    "\\tilde{o}' \\ne \\tilde{o}\n",
    "$$\n",
    "\n",
    "which indicates the effect of adding positional encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGPOxuD-8UYN"
   },
   "source": [
    "Same example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1679936380152,
     "user": {
      "displayName": "Chengbo Zang",
      "userId": "02269230170074283916"
     },
     "user_tz": 240
    },
    "id": "CLj0kpeSxV7s",
    "outputId": "b136e2e1-2563-4fb2-926d-fa44d057a61e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct q & kv\n",
    "# you need to run through section 2 before excecuting this cell\n",
    "q = torch.randn((1, 4, dim)).double()\n",
    "kv = feat.flatten(2, 3).permute(0, 2, 1).double()\n",
    "dim = kv.shape[-1]\n",
    "\n",
    "# obtain a positional encoding\n",
    "p = pos.flatten(2, 3).permute(0, 2, 1)\n",
    "\n",
    "# permute it\n",
    "I = np.arange(kv.shape[1])\n",
    "J = I.copy()\n",
    "np.random.shuffle(J)\n",
    "kv_permuted = kv[:, J, :]\n",
    "\n",
    "# test (use dtype=double to avoid numeric errors)\n",
    "attention = MultiHeadAttention(dim, 8, 0).double()\n",
    "att = attention(q, kv + p, kv).detach()\n",
    "att_permuted = attention(q, kv_permuted + p, kv_permuted).detach()\n",
    "\n",
    "# are they equal?\n",
    "assert not torch.allclose(att_permuted, att)\n",
    "print('Absolute error:', torch.norm(att_permuted - att, p=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7T8BSyXOAtnQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "046b66792d614528be4c04c999c4e9bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "047ef40428954d0999957418bc9c8c3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04d6774a411840718e9f18480671a194": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "167b612e78274384afbf5437a0244c5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67c9e644b502431592319161c799e3f6",
      "max": 4592,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b31b799c77314b229dc10ac7b956c6c6",
      "value": 4592
     }
    },
    "1ca3cbb9bd9b45dda9c725bb1c2f4642": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e7e9347c91c44e9a1d2c6ec16cd8689": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26e5b622bc424436a4f39678948a4a68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "273009a284d74df69aaeb33a75aef9e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2accce33fa2c4a88964414e4053d751a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dfdfeef01754f50be422d5fa0c6f334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_577dd9a0aa1f4029a732f371f78c911b",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26e5b622bc424436a4f39678948a4a68",
      "value": 46830571
     }
    },
    "2ec936d23b3e49cbb38bdc1d75749e6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_500a8ad2ba3e4bf0ab2fade550166a3b",
       "IPY_MODEL_d4e82c4fb19f473a805f4a2603ce8403",
       "IPY_MODEL_bbbd476969224dc6b288801717bc3a37"
      ],
      "layout": "IPY_MODEL_046b66792d614528be4c04c999c4e9bd"
     }
    },
    "345d31326fc74969b96d12de96c5b3ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36fc7b8f14374f0f900b0d9637dc1108": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "38136311e80d466fb3d8730e0937242e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c80796a2ac746e2b34059eb3aae1b23",
      "placeholder": "​",
      "style": "IPY_MODEL_f4318be27fa5479bbb561a7a81ad29fd",
      "value": "100%"
     }
    },
    "39a95df279974e828591944c04929239": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b52625e436e4c00a1dcd1e248b5d0bb",
      "max": 166731871,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e2431d662ec461099efb657efc7bd5e",
      "value": 166731871
     }
    },
    "500a8ad2ba3e4bf0ab2fade550166a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1b0e431f21b49a2885e9dbd39d32f60",
      "placeholder": "​",
      "style": "IPY_MODEL_f82d3e4ef4df40998c18d1bb8b657bf5",
      "value": "Downloading (…)rocessor_config.json: 100%"
     }
    },
    "577dd9a0aa1f4029a732f371f78c911b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "581e4d41bcac4623840c4bf4902ebe8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_68e21d8e89e94c488c2b6f4e8764e06a",
       "IPY_MODEL_167b612e78274384afbf5437a0244c5f",
       "IPY_MODEL_76b42e32e1e742d98dd6130702e4e771"
      ],
      "layout": "IPY_MODEL_1e7e9347c91c44e9a1d2c6ec16cd8689"
     }
    },
    "5b52625e436e4c00a1dcd1e248b5d0bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67c9e644b502431592319161c799e3f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68e21d8e89e94c488c2b6f4e8764e06a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cee57e2d7d1a4f428738cf2b5f87df66",
      "placeholder": "​",
      "style": "IPY_MODEL_04d6774a411840718e9f18480671a194",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "76b42e32e1e742d98dd6130702e4e771": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e656d84216ce4b698b13e73d024aa586",
      "placeholder": "​",
      "style": "IPY_MODEL_273009a284d74df69aaeb33a75aef9e4",
      "value": " 4.59k/4.59k [00:00&lt;00:00, 90.7kB/s]"
     }
    },
    "779eae05a27c42e4938623717a239546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87b61c5e134047e2856ba0d87aa3e5dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_38136311e80d466fb3d8730e0937242e",
       "IPY_MODEL_2dfdfeef01754f50be422d5fa0c6f334",
       "IPY_MODEL_96644c80cb7f42a585b461d70556dfb5"
      ],
      "layout": "IPY_MODEL_2accce33fa2c4a88964414e4053d751a"
     }
    },
    "8c80796a2ac746e2b34059eb3aae1b23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e2431d662ec461099efb657efc7bd5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "945c04aac89c43c59d765a425edba9c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce72f74b09534bb79fcbad143b487a7b",
       "IPY_MODEL_39a95df279974e828591944c04929239",
       "IPY_MODEL_b90109fd18854933828a75fff92089eb"
      ],
      "layout": "IPY_MODEL_aecba2e612b3496ebde80add97b11bcf"
     }
    },
    "95c1ee0c2347419a85b9a1e7aaa14065": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "96644c80cb7f42a585b461d70556dfb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_345d31326fc74969b96d12de96c5b3ea",
      "placeholder": "​",
      "style": "IPY_MODEL_a6f80041dd8c4af08b081b464106bf86",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 92.5MB/s]"
     }
    },
    "a1b0e431f21b49a2885e9dbd39d32f60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6f80041dd8c4af08b081b464106bf86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aecba2e612b3496ebde80add97b11bcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1d4c4489da54385bcb7d25ebdef9a4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b31b799c77314b229dc10ac7b956c6c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b90109fd18854933828a75fff92089eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe3718d9dad34988b41699afdbe23e6a",
      "placeholder": "​",
      "style": "IPY_MODEL_95c1ee0c2347419a85b9a1e7aaa14065",
      "value": " 167M/167M [00:01&lt;00:00, 96.5MB/s]"
     }
    },
    "bbbd476969224dc6b288801717bc3a37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ca3cbb9bd9b45dda9c725bb1c2f4642",
      "placeholder": "​",
      "style": "IPY_MODEL_d79e9bb7ecbf45e481df0103f446fc9e",
      "value": " 274/274 [00:00&lt;00:00, 4.12kB/s]"
     }
    },
    "ce72f74b09534bb79fcbad143b487a7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_047ef40428954d0999957418bc9c8c3d",
      "placeholder": "​",
      "style": "IPY_MODEL_779eae05a27c42e4938623717a239546",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "cee57e2d7d1a4f428738cf2b5f87df66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4e82c4fb19f473a805f4a2603ce8403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1d4c4489da54385bcb7d25ebdef9a4a",
      "max": 274,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_36fc7b8f14374f0f900b0d9637dc1108",
      "value": 274
     }
    },
    "d79e9bb7ecbf45e481df0103f446fc9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e656d84216ce4b698b13e73d024aa586": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4318be27fa5479bbb561a7a81ad29fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f82d3e4ef4df40998c18d1bb8b657bf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe3718d9dad34988b41699afdbe23e6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
